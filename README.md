# Scrapy çˆ¬è™«å¼€å‘æ¨¡æ¿

[![Open in Dev Containers](https://img.shields.io/static/v1?label=Dev%20Containers&message=Open&color=blue&logo=visualstudiocode)](https://vscode.dev/redirect?url=vscode://ms-vscode-remote.remote-containers/cloneInVolume?url=https://github.com/microsoft/vscode-remote-try-python)

ä¸€ä¸ªåŠŸèƒ½å®Œå–„çš„ Scrapy çˆ¬è™«å¼€å‘æ¨¡æ¿ï¼Œæ”¯æŒ Dev Containers å’Œ GitHub Codespaces å¼€å‘ç¯å¢ƒã€‚

## ç‰¹æ€§

- ğŸš€ å¼€ç®±å³ç”¨çš„ Scrapy çˆ¬è™«æ¡†æ¶
- ğŸ³ æ”¯æŒ Docker Dev Containers å¼€å‘ç¯å¢ƒ
- ğŸ’» å®Œå–„çš„ VS Code è°ƒè¯•é…ç½®
- ğŸ“¦ é¢„ç½®å¸¸ç”¨çˆ¬è™«ä¸­é—´ä»¶å’Œç®¡é“
- ğŸ“ ä¸°å¯Œçš„ç¤ºä¾‹çˆ¬è™«ä»£ç 
- ğŸ“š è¯¦ç»†çš„å¼€å‘æ–‡æ¡£

## å¿«é€Ÿå¼€å§‹

### ä½¿ç”¨ GitHub Codespaces

1. ç‚¹å‡»ä»“åº“çš„ **Code** ä¸‹æ‹‰èœå•
2. ç‚¹å‡» **Codespaces** æ ‡ç­¾
3. ç‚¹å‡» **Create codespace on main**

### ä½¿ç”¨ VS Code Dev Containers

å¦‚æœä½ å·²ç»å®‰è£…äº† VS Code å’Œ Dockerï¼š

1. å…‹éš†ä»“åº“åˆ°æœ¬åœ°
2. åœ¨ VS Code ä¸­æ‰“å¼€ä»“åº“
3. æŒ‰ `F1` å¹¶é€‰æ‹© **Dev Containers: Reopen in Container**
4. ç­‰å¾…å®¹å™¨æ„å»ºå®Œæˆ

### æœ¬åœ°å¼€å‘

ç¡®ä¿å·²å®‰è£… Python 3.8+ï¼š

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è¿è¡Œç¤ºä¾‹çˆ¬è™«
scrapy crawl example

# æˆ–ä½¿ç”¨ä¾¿æ·è„šæœ¬
python run.py example
```

## é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ crawler/                    # çˆ¬è™«æ ¸å¿ƒç›®å½•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ settings.py           # Scrapy é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ items.py             # æ•°æ®æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ pipelines.py         # æ•°æ®å¤„ç†ç®¡é“
â”‚   â”œâ”€â”€ middlewares.py       # ä¸­é—´ä»¶
â”‚   â”œâ”€â”€ utils.py             # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ exporters.py         # è‡ªå®šä¹‰å¯¼å‡ºå™¨
â”‚   â””â”€â”€ spiders/             # çˆ¬è™«ç›®å½•
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ example.py       # åŸºç¡€ç¤ºä¾‹çˆ¬è™«
â”‚       â”œâ”€â”€ async_spider.py  # å¼‚æ­¥çˆ¬è™«ç¤ºä¾‹
â”‚       â””â”€â”€ selenium_spider.py # Selenium çˆ¬è™«ç¤ºä¾‹
â”œâ”€â”€ output/                   # æ•°æ®è¾“å‡ºç›®å½•
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ docs/                     # æ–‡æ¡£ç›®å½•
â”‚   â”œâ”€â”€ GETTING_STARTED.md   # å¿«é€Ÿå¼€å§‹æŒ‡å—
â”‚   â”œâ”€â”€ BEST_PRACTICES.md    # æœ€ä½³å®è·µ
â”‚   â””â”€â”€ API_REFERENCE.md     # API å‚è€ƒ
â”œâ”€â”€ .devcontainer/            # Dev Container é…ç½®
â”œâ”€â”€ .vscode/                  # VS Code é…ç½®
â”œâ”€â”€ scrapy.cfg               # Scrapy é¡¹ç›®é…ç½®
â”œâ”€â”€ run.py                   # ä¾¿æ·è¿è¡Œè„šæœ¬
â”œâ”€â”€ requirements.txt         # Python ä¾èµ–
â””â”€â”€ README.md               # é¡¹ç›®è¯´æ˜
```

## ä½¿ç”¨ç¤ºä¾‹

### è¿è¡Œçˆ¬è™«

```bash
# ä½¿ç”¨ Scrapy å‘½ä»¤
scrapy crawl example

# ä½¿ç”¨ä¾¿æ·è„šæœ¬
python run.py example

# è°ƒè¯•æ¨¡å¼è¿è¡Œ
scrapy crawl example -s LOG_LEVEL=DEBUG

# ä¿å­˜æ•°æ®åˆ°æ–‡ä»¶
scrapy crawl example -o output/example.json
```

### åˆ›å»ºæ–°çˆ¬è™«

```bash
# ä½¿ç”¨ Scrapy å‘½ä»¤ç”Ÿæˆçˆ¬è™«æ¨¡æ¿
scrapy genspider myspider example.com

# æˆ–æ‰‹åŠ¨åˆ›å»ºæ–‡ä»¶
# åœ¨ crawler/spiders/ ç›®å½•ä¸‹åˆ›å»ºæ–°çš„çˆ¬è™«æ–‡ä»¶
```

### è°ƒè¯•çˆ¬è™«

åœ¨ VS Code ä¸­ï¼š

1. æ‰“å¼€ `crawler/spiders/example.py`
2. åœ¨ä»£ç è¡Œå·å·¦ä¾§ç‚¹å‡»è®¾ç½®æ–­ç‚¹
3. æŒ‰ `F5` æˆ–é€‰æ‹© "Scrapy" è°ƒè¯•é…ç½®
4. ç­‰å¾…æ–­ç‚¹å‘½ä¸­ï¼Œå¼€å§‹è°ƒè¯•

### ä½¿ç”¨ Scrapy Shell

```bash
# äº¤äº’å¼è°ƒè¯•
scrapy shell "https://example.com"

# ä½¿ç”¨ VS Code è°ƒè¯•é…ç½®
# é€‰æ‹© "Scrapy Shell" é…ç½®å¯åŠ¨
```

## é…ç½®è¯´æ˜

### åŸºç¡€é…ç½® ([`crawler/settings.py`](crawler/settings.py))

ä¸»è¦é…ç½®é¡¹ï¼š

- `BOT_NAME` - çˆ¬è™«åç§°
- `SPIDER_MODULES` - çˆ¬è™«æ¨¡å—è·¯å¾„
- `USER_AGENT` - ç”¨æˆ·ä»£ç†
- `ROBOTSTXT_OBEY` - æ˜¯å¦éµå®ˆ robots.txt
- `CONCURRENT_REQUESTS` - å¹¶å‘è¯·æ±‚æ•°
- `DOWNLOAD_DELAY` - ä¸‹è½½å»¶è¿Ÿ
- `ITEM_PIPELINES` - å¯ç”¨çš„æ•°æ®ç®¡é“
- `DOWNLOADER_MIDDLEWARES` - å¯ç”¨çš„ä¸‹è½½ä¸­é—´ä»¶

### ä¸­é—´ä»¶ ([`crawler/middlewares.py`](crawler/middlewares.py))

å†…ç½®ä¸­é—´ä»¶ï¼š

- `RandomUserAgentMiddleware` - éšæœº User-Agent
- `ProxyMiddleware` - ä»£ç†æ”¯æŒ
- `RetryMiddleware` - è¯·æ±‚é‡è¯•

### æ•°æ®ç®¡é“ ([`crawler/pipelines.py`](crawler/pipelines.py))

å†…ç½®ç®¡é“ï¼š

- `DataCleaningPipeline` - æ•°æ®æ¸…æ´—
- `FileSavePipeline` - æ–‡ä»¶ä¿å­˜
- `DeduplicationPipeline` - æ•°æ®å»é‡
- `ValidationPipeline` - æ•°æ®éªŒè¯

## ç¤ºä¾‹çˆ¬è™«

### åŸºç¡€çˆ¬è™« ([`example.py`](crawler/spiders/example.py))

çˆ¬å– https://example.com çš„åŸºç¡€ç¤ºä¾‹ï¼š

```python
import scrapy
from crawler.items import CrawlerItem


class ExampleSpider(scrapy.Spider):
    name = "example"
    allowed_domains = ["example.com"]
    start_urls = ["https://example.com"]

    def parse(self, response):
        item = ClawerItem()
        item['url'] = response.url
        item['title'] = response.css('h1::text').get()
        item['content'] = response.css('p::text').getall()
        yield item
```

### å¼‚æ­¥çˆ¬è™« ([`async_spider.py`](crawler/spiders/async_spider.py))

ä½¿ç”¨ aiohttp çš„å¼‚æ­¥çˆ¬è™«ç¤ºä¾‹ã€‚

### Selenium çˆ¬è™« ([`selenium_spider.py`](crawler/spiders/selenium_spider.py))

ä½¿ç”¨ Selenium å¤„ç† JavaScript æ¸²æŸ“é¡µé¢çš„ç¤ºä¾‹ã€‚

## å¸¸è§é—®é¢˜

### Q: å¦‚ä½•ä¿®æ”¹çˆ¬è™«çš„å¹¶å‘æ•°ï¼Ÿ

A: åœ¨ [`crawler/settings.py`](crawler/settings.py) ä¸­ä¿®æ”¹ `CONCURRENT_REQUESTS` é…ç½®é¡¹ã€‚

### Q: å¦‚ä½•æ·»åŠ ä»£ç†ï¼Ÿ

A: åœ¨ [`crawler/settings.py`](crawler/settings.py) ä¸­å¯ç”¨ `ProxyMiddleware` å¹¶é…ç½®ä»£ç†åˆ—è¡¨ã€‚

### Q: æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“ï¼Ÿ

A: åœ¨ [`crawler/pipelines.py`](crawler/pipelines.py) ä¸­æ·»åŠ æ•°æ®åº“ç®¡é“ï¼Œæˆ–åœ¨ `ITEM_PIPELINES` ä¸­é…ç½®ã€‚

### Q: å¦‚ä½•å¤„ç†ç™»å½•è®¤è¯ï¼Ÿ

A: åœ¨çˆ¬è™«çš„ `start_requests` æ–¹æ³•ä¸­æ·»åŠ ç™»å½•é€»è¾‘ï¼Œæˆ–ä½¿ç”¨ `FormRequest` å‘é€ç™»å½•è¯·æ±‚ã€‚

## å¼€å‘æŒ‡å—

è¯¦ç»†çš„å¼€å‘æŒ‡å—è¯·å‚è€ƒï¼š

- [å¿«é€Ÿå¼€å§‹æŒ‡å—](docs/GETTING_STARTED.md)
- [æœ€ä½³å®è·µ](docs/BEST_PRACTICES.md)
- [API å‚è€ƒ](docs/API_REFERENCE.md)

## æŠ€æœ¯æ ˆ

- **Scrapy** - çˆ¬è™«æ¡†æ¶
- **BeautifulSoup4** - HTML è§£æ
- **Requests** - HTTP è¯·æ±‚
- **lxml** - XML/HTML è§£æ
- **Selenium** - æµè§ˆå™¨è‡ªåŠ¨åŒ–
- **aiohttp** - å¼‚æ­¥ HTTP å®¢æˆ·ç«¯

## è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## é“¾æ¥

- [Scrapy å®˜æ–¹æ–‡æ¡£](https://docs.scrapy.org/)
- [Scrapy GitHub](https://github.com/scrapy/scrapy)
- [VS Code Dev Containers](https://code.visualstudio.com/docs/devcontainers/containers)
